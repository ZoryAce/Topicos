{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZoryAce/Topicos/blob/main/Taller4_PretrainedModelsPytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XMpXRys76MZ"
      },
      "source": [
        "# Tranfer Learnning to Flower Recognition using PyTorch 🔥\n",
        "This dataset contains labeled 4242 images of flowers.\n",
        "\n",
        "\n",
        "### Content\n",
        "The pictures are divided into five classes: chamomile, tulip, rose, sunflower, dandelion.\n",
        "\n",
        "For each class there are about 800 photos. Photos are not high resolution, about 320x240 pixels. Photos are not reduced to a single size, they have different proportions!\n",
        "\n",
        "## Data\n",
        "You can download data from: [Flowers Recognition Dataset](https://www.kaggle.com/datasets/alxmamaev/flowers-recognition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ePBa7UeC742g",
        "outputId": "8983bcfc-db6c-416a-d73f-cbd0d190781c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'dataset.zip', 'flowers.zip', 'dataset', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "path=\"./\"\n",
        "os.listdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gdown"
      ],
      "metadata": {
        "id": "U-N2YbUO3nTm",
        "outputId": "5d0addb7-0a9c-4207-9cb1-38d02f7c951b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# ID del archivo extraído del enlace de Google Drive\n",
        "file_id = \"15b0wfUXZZiPd3RAnt_quJZIjW3kP6mAA\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "# Descargar el archivo\n",
        "output = \"flowers.zip\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Descomprimir el archivo\n",
        "ruta_destino = '/content/'\n",
        "os.makedirs(ruta_destino, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall(ruta_destino)\n",
        "\n",
        "print(\"¡Archivo descargado y descomprimido con éxito!\")\n",
        "\n",
        "\n",
        "output = \"dataset.zip\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Descomprimir el archivo\n",
        "ruta_destino = '/content/dataset/'\n",
        "os.makedirs(ruta_destino, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall(ruta_destino)\n",
        "\n",
        "print(\"¡Archivo descargado y descomprimido con éxito!\")\n"
      ],
      "metadata": {
        "id": "c_g46j253vFY",
        "outputId": "bda57554-7993-41b6-dfdc-bb7b6198db31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=15b0wfUXZZiPd3RAnt_quJZIjW3kP6mAA\n",
            "From (redirected): https://drive.google.com/uc?id=15b0wfUXZZiPd3RAnt_quJZIjW3kP6mAA&confirm=t&uuid=3ab2c1e2-12ce-49b9-b226-1c5c6d0565e5\n",
            "To: /content/flowers.zip\n",
            "\n",
            "  0%|          | 0.00/166M [00:00<?, ?B/s]\u001b[A\n",
            "  9%|▉         | 15.2M/166M [00:00<00:00, 151MB/s]\u001b[A\n",
            " 21%|██        | 35.1M/166M [00:00<00:00, 178MB/s]\u001b[A\n",
            " 33%|███▎      | 55.1M/166M [00:00<00:00, 186MB/s]\u001b[A\n",
            " 45%|████▍     | 74.4M/166M [00:00<00:00, 189MB/s]\u001b[A\n",
            " 57%|█████▋    | 93.8M/166M [00:00<00:00, 181MB/s]\u001b[A\n",
            " 68%|██████▊   | 112M/166M [00:00<00:00, 182MB/s] \u001b[A\n",
            " 79%|███████▉  | 131M/166M [00:00<00:00, 182MB/s]\u001b[A\n",
            "100%|██████████| 166M/166M [00:00<00:00, 186MB/s]\n",
            " 48%|████▊     | 79.2M/166M [00:24<00:27, 3.18MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Archivo descargado y descomprimido con éxito!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=15b0wfUXZZiPd3RAnt_quJZIjW3kP6mAA\n",
            "From (redirected): https://drive.google.com/uc?id=15b0wfUXZZiPd3RAnt_quJZIjW3kP6mAA&confirm=t&uuid=74bae235-f1f9-4956-b565-26099de47cdb\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 166M/166M [00:01<00:00, 115MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Archivo descargado y descomprimido con éxito!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UQdfL32z_BWO"
      },
      "outputs": [],
      "source": [
        "classes=['dandelion', 'daisy', 'sunflower', 'tulip', 'rose']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3VsLxwfHCbP5",
        "outputId": "8fa4af44-31f0-44a3-fdf1-d8e624841d59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de imagenes: 5492\n"
          ]
        }
      ],
      "source": [
        "#CONTEO DE IMAGENES EN EL ARCHIVO LA CUAL DIFIERE DE LO INDICADO\n",
        "#EN EL ENCABEZADO (4317 IMAGENES)\n",
        "img_type = ('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')\n",
        "img_qty = sum([len(files) for r, d, files in os.walk(path)\n",
        "                   if any(file.lower().endswith(img_type) for file in files)])\n",
        "\n",
        "print(f\"Número de imagenes: {img_qty}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Idt0FAef_FSL",
        "outputId": "88d38a5f-d4d8-481d-b1f2-0f7d3228d07b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './dandelion'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b582121e8168>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mkind_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mimg_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlabel_of_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dandelion'"
          ]
        }
      ],
      "source": [
        "label_of_file=[]\n",
        "img_list=[]\n",
        "\n",
        "for kind in classes:\n",
        "    kind_path=os.path.join(path,kind)\n",
        "\n",
        "    for img in os.listdir(kind_path):\n",
        "        img_list.append(os.path.join(kind_path,img))\n",
        "        label_of_file.append(kind)\n",
        "\n",
        "df=pd.DataFrame({'img':img_list,'label':label_of_file})\n",
        "df.head(3)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA7bAX2P_H1V"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "df['encode_label'] = encoder.fit_transform(df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6bm8oMo_Qd0"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okJJG8No_SgX"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
        "from torchvision import transforms, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNKSyejg_fpT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(example_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha9681z3_Zw9"
      },
      "outputs": [],
      "source": [
        "example_img = cv2.imread(df['img'][2])\n",
        "example_img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlQpuDUz5L9V"
      },
      "outputs": [],
      "source": [
        "example_img_rgb = cv2.cvtColor(example_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(example_img_rgb)\n",
        "plt.axis('off')  # Opcional: oculta los ejes\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVKrJowd_lUY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "x=[]\n",
        "for img in df['img']:\n",
        "    try:\n",
        "        img = cv2.imread(img)\n",
        "        img = cv2.resize(img, (150, 150))\n",
        "        img = img / 255.0  # normalize\n",
        "        x.append(img)      # img\n",
        "\n",
        "    except:\n",
        "        print(f\"Error loading image: {img}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "064dSpPX_-qN"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6OpGOW28KSN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convertir a tensores x y y\n",
        "X_tensor = torch.tensor(np.array(x).transpose(0, 3, 1, 2), dtype=torch.float32)  # Formato (N, C, H, W)\n",
        "y_tensor = torch.tensor(df['encode_label'].values, dtype=torch.long)              # Etiquetas como tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtBEgxES_BnY"
      },
      "outputs": [],
      "source": [
        "df_tensor = TensorDataset(X_tensor, y_tensor)#crear dataset de tensores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB6_hpvnH3Cw"
      },
      "outputs": [],
      "source": [
        "#Creacion dataset entrenamiento y pruebas\n",
        "train_size = int(0.8 * len(df_tensor))\n",
        "test_size = len(df_tensor) - train_size\n",
        "train_dataset, test_dataset = random_split(df_tensor, [train_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koa-V1B1AWj7"
      },
      "outputs": [],
      "source": [
        "# Creación de dataloader de entrenamiento y pruebas\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R09Z8ok_4nq"
      },
      "outputs": [],
      "source": [
        "# make model\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_pGstGtA06r"
      },
      "outputs": [],
      "source": [
        "# train Model\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzf8ppD-A4t9"
      },
      "outputs": [],
      "source": [
        "# train model\n",
        "def train_model(model, criterion, optimizer, num_epochs=25):\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in zip(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      inputs = inputs.double()\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item() * inputs.size(0)\n",
        "      avg_loss = running_loss / len(train_loader.dataset)\n",
        "      print(f\"Época {epoch+1}/{num_epochs} - Pérdida: {avg_loss:.4f}\")\n",
        "    print(\"Entrenamiento completado ✅\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcau5NgTJJyJ"
      },
      "outputs": [],
      "source": [
        "# train model chat\n",
        "def train_model(model, criterion, optimizer, num_epochs=25):\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    # Iterate directly over the train_loader\n",
        "    for inputs, labels in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      #inputs = inputs.double()\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item() * inputs.size(0)\n",
        "      avg_loss = running_loss / len(train_loader.dataset)\n",
        "      print(f\"Época {epoch+1}/{num_epochs} - Pérdida: {avg_loss:.4f}\")\n",
        "    print(\"Entrenamiento completado ✅\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70QkOQoHCEND"
      },
      "outputs": [],
      "source": [
        "# Test Model\n",
        "def test_model(model):\n",
        "  model.eval()\n",
        "  test_loss = 0.0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in zip(test_loader):\n",
        "      # Permute the dimensions of the input tensor\n",
        "      inputs = inputs.permute(0, 3, 1, 2) # Change the order of dimensions from [batch_size, height, width, channels] to [batch_size, channels, height, width]\n",
        "      outputs = model(inputs)\n",
        "      test_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
        "      pred = outputs.argmax(dim=1, keepdim=True)\n",
        "      correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "  test_loss /= len(X_train.dataset)\n",
        "  test_accuracy = 100. * correct / len(X_train.dataset)\n",
        "  print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(X_train.dataset)} ({test_accuracy:.0f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2HidwK2I1jn"
      },
      "outputs": [],
      "source": [
        "#chat\n",
        "def test_model(model, test_loader, criterion):\n",
        "    model.eval()  # Modo evaluación\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:  # Iteración correcta\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            test_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
        "\n",
        "            pred = outputs.argmax(dim=1, keepdim=True)  # Predicción más probable\n",
        "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print(f'Test set: Average loss: {test_loss:.4f}, '\n",
        "          f'Accuracy: {correct}/{len(test_loader.dataset)} ({test_accuracy:.0f}%)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-1AUnkvAHGu"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjNNJgiFAMGh"
      },
      "outputs": [],
      "source": [
        "# training\n",
        "train_model(model, criterion, optimizer, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqjMpz60APr-"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EftRkiiJx4KI"
      },
      "outputs": [],
      "source": [
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():  # Desactiva el cálculo de gradientes\n",
        "    for images, labels in test_loader:\n",
        "        #images, labels = images.to(device), labels.to(device)  # GPU si está disponible\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calcular la pérdida\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Calcular precisión\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "# Mostrar resultados\n",
        "print(f'Pérdida en test: {test_loss / len(test_loader):.4f}')\n",
        "print(f'Precisión en test: {100 * correct / total:.2f}%')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}